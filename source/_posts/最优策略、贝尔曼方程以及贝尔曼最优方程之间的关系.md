---
title: 最优策略、贝尔曼方程以及贝尔曼最优方程之间的关系
date: 2020-07-22 14:31:48
tags: RL
---

<!-- toc -->
<!--more-->

#### 贝尔曼方程

- 状态值函数贝尔曼方程  
$$V_{\pi}(s)=\sum_{a} \pi(a \mid s) \sum_{s} p_{a}\left(s, s^{\prime}\right)\left(R_{a}\left(s, s^{\prime}\right)+\gamma V_{\pi}\left(s^{\prime}\right)\right)$$

- 动作值函数贝尔曼方程  
$$Q_{\pi}(s, a)=\sum_{s} p_{a}\left(s, s^{\prime}\right)\left(R_{a}\left(s, s^{\prime}\right)+\gamma V_{\pi}\left(s^{\prime}\right)\right)$$

#### 最优策略  
最优策略$\pi$要满足以下条件，在任意状态$s$，对任意其他状态$\pi'$,都有：  
$$V_{\pi}^{*}(s) \geq V_{\pi'}(s)$$

**所有的最优策略有相同的状态价值函数和动作价值函数值**  

最优动作值函数定义为：  
$$Q^{*}(s, a)=\max _{\pi} Q_{\pi}(s, a)$$

对于状态-动作对(s,a)，最优动作价值函数给出了在状态s时执行动作a，后续状态时按照最优策略执行时的预期回报。找到了最优动作价值函数，根据它可以得到最优策略，具体做法是在每个状态时执行动作价值函数值最大的那个动作：    
$$\pi^{*}(s)=\arg \max _{a} Q^{*}(s, a)$$

#### 贝尔曼最优方程

- 状态价值函数  
$$V^{*}(s)=\max _{a} \sum_{s'} p_{a}\left(s, s^{\prime}\right)\left(R_{a}\left(s, s^{\prime}\right)+\gamma V^{*}\left(s^{\prime}\right)\right)$$

上式的意义是对任何一个状态s，要保证一个策略π能让状态价值函数取得最大值，则需要本次执行的动作a所带来的回报与下一状态s'的最优状态价值函数值之和是最优的。

- 动作值函数  
$$Q^{*}(s, a)=\sum_{s'} p_{a}\left(s, s^{\prime}\right)\left(R_{a}\left(s, s^{\prime}\right)+\gamma \max _{a} Q^{*}\left(s^{\prime}, a^{\prime}\right)\right)$$

其意义是要保证一个策略使得动作价值函数是最优的，则需要保证在执行完本动a之后，在下一个状态s'所执行的动作a'是最优的

对于任意有限状态和动作的马尔可夫决策过程，贝尔曼最优方程有唯一解，且与具体的策略无关。可以将贝尔曼最优性方程看成一个方程组，每个状态有一个方程，未知数的数量也等于状态的数量。



